{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb8ed86-ff33-4719-804a-b6c255667970",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c47ef-b6d1-4131-bcbd-826ce93906f0",
   "metadata": {},
   "source": [
    "https://youtu.be/Dh9CL8fyG80\n",
    "\n",
    "Now we'll see how to achieve the same results as we did in the last section without using the Trainer class. Again, we assume you have done the data processing in section 2. Here is a short summary covering evreything you will need:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56435e24-d89d-4e8a-b142-b161129ba960",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ac682-beb1-4c7c-90f6-c6b524efb4c8",
   "metadata": {},
   "source": [
    "## Prepare for training\n",
    "\n",
    "Before actually writing our training loop, we will need to define a few objectives. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our tokenized_datasets, to take care of some things that the Trainer did for use automatically. Specifically, we need to:\n",
    "\n",
    "- Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "- Rename the column label to labels (because the model expects the argument to be named labels).\n",
    "- Set the format of the datasets so they return PyTorch tensors instead of lists.\n",
    "\n",
    "Ou tokenized_datasets has one method for each of those steps:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "578e9f28-593a-4d30-a552-cbed13a77af7",
   "metadata": {},
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc43c1-b550-41e5-a105-9b9e40d1f45c",
   "metadata": {},
   "source": [
    "We can then check that the result only has columns that our model will accept:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "598d9433-4ebf-4a44-a58b-667e0b3045ea",
   "metadata": {},
   "source": [
    "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f155b-8475-4909-9f84-a4909b4c66f5",
   "metadata": {},
   "source": [
    "Now that this is done, we can easily define our dataloaders:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d876d1b-620b-4aaf-9f38-d9607071d1a6",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e268e-c8ca-419c-9901-b56131094032",
   "metadata": {},
   "source": [
    "To quickly check there is no mistake in the data processing, we can inspect a batch like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2b32397-4006-4270-8ddc-63c4d87c9fcd",
   "metadata": {},
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4888d1cb-0f5e-4147-ae81-71ce0594bfe6",
   "metadata": {},
   "source": [
    "{'attention_mask': torch.Size([8, 65]),\n",
    " 'input_ids': torch.Size([8, 65]),\n",
    " 'labels': torch.Size([8]),\n",
    " 'token_type_ids': torch.Size([8, 65])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8a79c-1c70-4d7b-8c03-82a6d54297ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
