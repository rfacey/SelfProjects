{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a495474-4b63-484f-9fea-977365a88c71",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6245afc-aa84-46f4-a292-8a7f76b970dc",
   "metadata": {},
   "source": [
    "# Processing the data\n",
    "\n",
    "Continuing with the example from the previous chapter, here is how we would train a sequence classifier on one batch in PyTorch:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a67f22c6-1b17-428c-80c4-9a86d0443873",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c47ba7-f005-4abe-8020-4ea8a1078af0",
   "metadata": {},
   "source": [
    "*Note: A lot of these items will be done in the HuggingFace website and not ran in JupyterLabs.*\n",
    "\n",
    "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.\n",
    "\n",
    "In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper (https://www.aclweb.org/anthology/I05-5002.pdf) by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We've selected it for this chapter because it's a small dataset, so it's easy to experiment with training on it.\n",
    "\n",
    "## Loading a dataset from the Hub\n",
    "\n",
    "https://youtu.be/_BZearw7f0w\n",
    "\n",
    "The Hub doesn't just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets here (https://huggingface.co/datasets), and we recommend you try to load and process a new dataset once you have gone through this section (see the general documention here: https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub). But for now, let's focus on the MRPC dataset! This is one of the 10 datasets composing the GLUE benchmark (https://gluebenchmark.com/), which is an academic benchmark that is used ot measure the performance of ML models across 10 different text classification tasks.\n",
    "\n",
    "The ðŸ¤— Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67f31960-69a6-46f6-992f-214bf29c4aa4",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "245ce2aa-c572-4bbb-b020-2abfc152c6f8",
   "metadata": {},
   "source": [
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 3668\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 408\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
    "        num_rows: 1725\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cad017-53fe-415c-83cd-0e724f9668a4",
   "metadata": {},
   "source": [
    "As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the text set).\n",
    "\n",
    "This command downloads and caches the dataset, by default in *~/.cache/huggingface/datasets*. Recall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "770e5b50-9c07-4357-b1de-a302af42e597",
   "metadata": {},
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "613d4f37-c5d4-4e07-a3ab-e1d545a2f4a2",
   "metadata": {},
   "source": [
    "{'idx': 0,\n",
    " 'label': 1,\n",
    " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
    " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08195608-ba04-4589-875e-34713322ee4b",
   "metadata": {},
   "source": [
    "We can see the labels are already integers, so we won't have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell use the type of each column:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e1e81fd-3382-4164-8504-65d2d19a84cd",
   "metadata": {},
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0b0fda4-efc0-460d-89af-588aefb15761",
   "metadata": {},
   "source": [
    "{'sentence1': Value(dtype='string', id=None),\n",
    " 'sentence2': Value(dtype='string', id=None),\n",
    " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
    " 'idx': Value(dtype='int32', id=None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eb919-dc56-4bf3-a8e0-304ba41b6f8b",
   "metadata": {},
   "source": [
    "Behind the scenes, label is of the type ClassLabel, and the mapping of integers to label name is stored in the *names* folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent.\n",
    "\n",
    "## Preprocessing a dataset\n",
    "\n",
    "https://youtu.be/0u3ioSwev3s\n",
    "\n",
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the previous chapter, this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83adee54-df85-4dc7-bdb4-2c535e0af311",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da430b66-1931-4e4d-a47c-353275ba81d7",
   "metadata": {},
   "source": [
    "However, we can't just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a84f4abd-ecdc-4115-bbb7-45a1846af4c7",
   "metadata": {},
   "source": [
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4dd4a6-a035-4ed9-8444-e4f189ad7ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
