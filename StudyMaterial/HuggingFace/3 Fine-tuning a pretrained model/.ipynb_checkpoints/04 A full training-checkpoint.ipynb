{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb8ed86-ff33-4719-804a-b6c255667970",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c47ef-b6d1-4131-bcbd-826ce93906f0",
   "metadata": {},
   "source": [
    "https://youtu.be/Dh9CL8fyG80\n",
    "\n",
    "Now we'll see how to achieve the same results as we did in the last section without using the Trainer class. Again, we assume you have done the data processing in section 2. Here is a short summary covering evreything you will need:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56435e24-d89d-4e8a-b142-b161129ba960",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ac682-beb1-4c7c-90f6-c6b524efb4c8",
   "metadata": {},
   "source": [
    "## Prepare for training\n",
    "\n",
    "Before actually writing our training loop, we will need to define a few objectives. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our tokenized_datasets, to take care of some things that the Trainer did for use automatically. Specifically, we need to:\n",
    "\n",
    "- Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
    "- Rename the column label to labels (because the model expects the argument to be named labels).\n",
    "- Set the format of the datasets so they return PyTorch tensors instead of lists.\n",
    "\n",
    "Ou tokenized_datasets has one method for each of those steps:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "578e9f28-593a-4d30-a552-cbed13a77af7",
   "metadata": {},
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc43c1-b550-41e5-a105-9b9e40d1f45c",
   "metadata": {},
   "source": [
    "We can then check that the result only has columns that our model will accept:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "598d9433-4ebf-4a44-a58b-667e0b3045ea",
   "metadata": {},
   "source": [
    "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f155b-8475-4909-9f84-a4909b4c66f5",
   "metadata": {},
   "source": [
    "Now that this is done, we can easily define our dataloaders:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d876d1b-620b-4aaf-9f38-d9607071d1a6",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e268e-c8ca-419c-9901-b56131094032",
   "metadata": {},
   "source": [
    "To quickly check there is no mistake in the data processing, we can inspect a batch like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2b32397-4006-4270-8ddc-63c4d87c9fcd",
   "metadata": {},
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4888d1cb-0f5e-4147-ae81-71ce0594bfe6",
   "metadata": {},
   "source": [
    "{'attention_mask': torch.Size([8, 65]),\n",
    " 'input_ids': torch.Size([8, 65]),\n",
    " 'labels': torch.Size([8]),\n",
    " 'token_type_ids': torch.Size([8, 65])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67572223-a83a-49ee-ae03-f84a38e00c56",
   "metadata": {},
   "source": [
    "Note that the actual shapes will probably be slightly different for you since we set shuffle=True for the training dataloader and we are padding to the maximum length inside the batch.\n",
    "\n",
    "Now that we're completely finished the data preprocessing (a satisfying yet elusive goal for any ML practitioner), let's turn to the model. We instantiate it exactly as we did in the previous section:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e64ae2da-df4e-43b5-aad4-a2ee75319be0",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf8db3-7bb0-4c40-b5d2-5f54985b1051",
   "metadata": {},
   "source": [
    "To make sure that everything will go smoothly during training, we pass our batch to this model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fea4cd8a-b066-49a9-ad17-f9e953af448e",
   "metadata": {},
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0877246d-5f14-46a5-b803-c90549a7d035",
   "metadata": {},
   "source": [
    "tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b39cd-8c3d-48da-83e4-ff3bd4f1b3b9",
   "metadata": {},
   "source": [
    "All ðŸ¤— Transformers models will return the loss when labels are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2).\n",
    "\n",
    "We're almost ready to write our training loop! We're just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the Trainer was doing by hand, we will use the same defaults. The optimizer used by the Trainer is AdamW, which is the same as Adam, but with a twist for weight decay regularization (see Decoupled Weight Decay Regularization by Ilya Loshchilov and Frank Hutter: https://arxiv.org/abs/1711.05101)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43af883b-b89d-4673-8294-72ff06f1ba88",
   "metadata": {},
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bb505-2476-49be-9d3a-4fd303cf545a",
   "metadata": {},
   "source": [
    "Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The Trainer uses three epochs by default, so we will follow that:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f40f1719-5eaa-40cd-a593-706f5e5d12cf",
   "metadata": {},
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6779591-3e1c-4faa-b36d-2b26c43217c1",
   "metadata": {},
   "source": [
    "1377"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882f21e-0573-405e-918b-0dc326856a97",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a device we will put our model and our batches on:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9654f71b-0b57-47a3-a4a6-b5a221900adb",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc0b8d5a-c5ba-4108-8e35-cbaf50501674",
   "metadata": {},
   "source": [
    "device(type='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf1fc3-c06c-49d6-b246-1ee4a27612a5",
   "metadata": {},
   "source": [
    "We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f5e2dd0-a27c-47b1-a484-f380c49d97c9",
   "metadata": {},
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7718f3-807a-47a0-bf72-3d008877e280",
   "metadata": {},
   "source": [
    "You can see that the core of the training loops looks a lot like the one in the introduction. We didn't ask for any reporting, so this training loop will not tell us anything about how the model fares. We need to add an evaluation loop for that.\n",
    "\n",
    "## The evaluation loop\n",
    "\n",
    "As we did earlier, we will use a metric provided by the ðŸ¤— Evaluate library. We've already seen the metric.compute() method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method add_batch(). Once we ahve accumulated all the batches, we can get the final result with metric.compute(). Here's how to implement all of this in an evaluation loop:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a374cc-1c99-4a04-9a02-2bfce595d9b6",
   "metadata": {},
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2a19bd1-f202-454b-a87c-cad61f6e5090",
   "metadata": {},
   "source": [
    "{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19643f0f-11fc-4e98-bee5-fcba1c0d5f2b",
   "metadata": {},
   "source": [
    "Again, your results will be slightly different because of the randomness in the model head initialization and the data shuffling, but they should be in the same ballpark.\n",
    "\n",
    "## Supercharge your training loop with ðŸ¤— Accelerate\n",
    "\n",
    "https://youtu.be/s7dy8QRgjJ0\n",
    "\n",
    "The training loop we defined earlier works fine on a single CPU or GPU. But using the ðŸ¤— Accelerate (https://github.com/huggingface/accelerate) library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. Starting from the creation of the training and validation dataloaders, here is what our manual trianing loop looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75a0442e-bfa6-4a86-970a-e3df233887fc",
   "metadata": {},
   "source": [
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd252e45-3144-4999-a4b1-ac4efcab77b5",
   "metadata": {},
   "source": [
    "And here are the changes:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b14d592b-6027-4328-8775-217fae7f13be",
   "metadata": {},
   "source": [
    "+ from accelerate import Accelerator\n",
    "  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "+ accelerator = Accelerator()\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "- model.to(device)\n",
    "\n",
    "+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "+     train_dataloader, eval_dataloader, model, optimizer\n",
    "+ )\n",
    "\n",
    "  num_epochs = 3\n",
    "  num_training_steps = num_epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "      for batch in train_dataloader:\n",
    "-         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "-         loss.backward()\n",
    "+         accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3b060-01a8-42a6-bf76-15d596d7a74f",
   "metadata": {},
   "source": [
    "The first line to add is the import line. The second line instantiates the Accelerator object that will look athe environment and intializizes the proper distributed setup. ðŸ¤— Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use accelerator.device instead of device).\n",
    "\n",
    "Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimzer to accelerator.prepare(). This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the device (again, if you want to keep this you can just change it to use accelerator.device) and replacing loss.backward() with accelerator.backward(loss).\n",
    "\n",
    "If you'd like to copy and past it to play around, here's what the complete training loop looks like with ðŸ¤— Accelerate:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "893baf08-1b3e-4339-848f-f01278416682",
   "metadata": {},
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817cc94-8d58-45e6-ae74-6685b7982f8e",
   "metadata": {},
   "source": [
    "Putting this in a train.py script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d48ed8d0-aa10-43af-9a1a-784654107b79",
   "metadata": {},
   "source": [
    "accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435a418-2695-48cb-8a83-d1166cebb9f9",
   "metadata": {},
   "source": [
    "which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9db3025-c75c-41f0-aadd-f4302ee55608",
   "metadata": {},
   "source": [
    "accelerate launch train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c473e6-6d45-44a5-bfae-e66ffb0a84fa",
   "metadata": {},
   "source": [
    "which will launch the distributed training.\n",
    "\n",
    "If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a training_function() and run a last cell with:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e535039e-a709-4e25-8ddc-126b1ce52775",
   "metadata": {},
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8afdd62-5f76-4f9e-b2f2-cc26656efd6b",
   "metadata": {},
   "source": [
    "You can find more examples in the ðŸ¤— Accelerate repo: https://github.com/huggingface/accelerate/tree/main/examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
